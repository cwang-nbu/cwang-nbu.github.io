[{"authors":["2018_QM_Mao"],"categories":null,"content":"研究领域：零样本目标检测\n目前就职公司：京东物流\n在校期间获奖经历： 宁波市计算机优秀学生论文奖 国家奖学金 校级一等奖学金 三好研究生 “卓创”研究生科研成果奖 曹光彪研究生科研成果奖\n已发表论文： Q. Mao, C. Wang, S. Yu, Y. Zheng and Y. Li, “Zero-Shot Object Detection With Attributes-Based Category Similarity,” in IEEE Transactions on Circuits and Systems II: Express Briefs, vol. 67, no. 5, pp. 921-925, May 2020, doi: 10.1109/TCSII.2020.2982316.\n在投论文： Q. Mao, C. Wang, H. Li, S. Yu, X. Ye and J. Wu, “Pseudo-Label Quality Control for Zero-Shot Object Detection.”\n兴趣爱好：看小说、吃吃睡睡\n","date":1633046400,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1633046400,"objectID":"7b01e77c2a704b190bd054c84bc00449","permalink":"https://cwang-nbu.github.io/authors/2018_qm_mao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/2018_qm_mao/","section":"authors","summary":"研究领域：零样本目标检测 目前就职公司：京东物流 在校期间获奖经","tags":null,"title":"毛乔梅","type":"authors"},{"authors":["2018_SH_Yu"],"categories":null,"content":"自我介绍，包括研究领域、论文等 工作等\n","date":1633046400,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1633046400,"objectID":"a8539050fa5f4329ad781c253eb3cb2b","permalink":"https://cwang-nbu.github.io/authors/2018_sh_yu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/2018_sh_yu/","section":"authors","summary":"自我介绍，包括研究领域、论文等 工作等","tags":null,"title":"郁盛浩","type":"authors"},{"authors":["2019_WW_Chen"],"categories":null,"content":"自我介绍，包括研究领域、论文等\n","date":1616284800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1616371200,"objectID":"d2230b9106f4b46428f6a865b0db3f43","permalink":"https://cwang-nbu.github.io/authors/2019_ww_chen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/2019_ww_chen/","section":"authors","summary":"自我介绍，包括研究领域、论文等","tags":null,"title":"陈魏魏","type":"authors"},{"authors":["2019_HH_Li"],"categories":null,"content":"自我介绍，包括研究领域、论文等\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"9576e376b8fb2d8cb4a0e7b609bf233e","permalink":"https://cwang-nbu.github.io/authors/2019_hh_li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/2019_hh_li/","section":"authors","summary":"自我介绍，包括研究领域、论文等","tags":null,"title":"李浩和","type":"authors"},{"authors":["2019_WJ_Liu"],"categories":null,"content":"自我介绍，包括研究领域、论文等\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"bdf24dd65bf974fe8b826859f88f3158","permalink":"https://cwang-nbu.github.io/authors/2019_wj_liu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/2019_wj_liu/","section":"authors","summary":"自我介绍，包括研究领域、论文等","tags":null,"title":"刘伟杰","type":"authors"},{"authors":["2019_JY_Che"],"categories":null,"content":"自我介绍，包括研究领域、论文等\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"f58f62e205e2a4e6c0e402a619f5bcdb","permalink":"https://cwang-nbu.github.io/authors/2019_jy_che/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/2019_jy_che/","section":"authors","summary":"自我介绍，包括研究领域、论文等","tags":null,"title":"车皆咏","type":"authors"},{"authors":["2020_Z_Huo"],"categories":null,"content":"自我介绍，包括研究领域、论文等\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"ee35be1f236833056f0166fd7d7da159","permalink":"https://cwang-nbu.github.io/authors/2020_z_huo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/2020_z_huo/","section":"authors","summary":"自我介绍，包括研究领域、论文等","tags":null,"title":"霍铮","type":"authors"},{"authors":["2020_S_Chen"],"categories":null,"content":"自我介绍，包括研究领域、论文等\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"7db948a92f470f7cb1d4f0bed6778cb7","permalink":"https://cwang-nbu.github.io/authors/2020_s_chen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/2020_s_chen/","section":"authors","summary":"自我介绍，包括研究领域、论文等","tags":null,"title":"陈松","type":"authors"},{"authors":["2020_YL_Gong"],"categories":null,"content":"自我介绍，包括研究领域、论文等\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"e58af4933bafd29d7e745e7698e11b44","permalink":"https://cwang-nbu.github.io/authors/2020_yl_gong/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/2020_yl_gong/","section":"authors","summary":"自我介绍，包括研究领域、论文等","tags":null,"title":"龚益玲","type":"authors"},{"authors":["2021_YJ_Zheng"],"categories":null,"content":"自我介绍，包括研究领域、论文等\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"1ebb651b74352b65e6c163edc8cc489f","permalink":"https://cwang-nbu.github.io/authors/2021_yj_zheng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/2021_yj_zheng/","section":"authors","summary":"自我介绍，包括研究领域、论文等","tags":null,"title":"郑瑜杰","type":"authors"},{"authors":["2021_CC_Tao"],"categories":null,"content":"自我介绍，包括研究领域、论文等\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"964833972a1c7fa7ee1c0a8d8dc311be","permalink":"https://cwang-nbu.github.io/authors/2021_cc_tao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/2021_cc_tao/","section":"authors","summary":"自我介绍，包括研究领域、论文等","tags":null,"title":"陶晨晨","type":"authors"},{"authors":["2021_XM_Dai"],"categories":null,"content":"自我介绍，包括研究领域、论文等\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"f076452807785479d9abf67f02c224bf","permalink":"https://cwang-nbu.github.io/authors/2021_xm_dai/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/2021_xm_dai/","section":"authors","summary":"自我介绍，包括研究领域、论文等","tags":null,"title":"戴鑫淼","type":"authors"},{"authors":["admin"],"categories":null,"content":"王翀，博士，副教授，现任 宁波大学 信息科学与工程学院 计算机科学与技术系 副主任。主要从事计算机视觉、图像/视频处理、人工智能相关的理论与应用的研究，研究领域包括：手势识别、人机交互系统、深度学习、目标检测/识别，行为识别、深度图像处理、基于图像的渲染等。\n以第一作者身份（通讯作者），发表SCI/EI论文20篇，参与/合作发表SCI/EI论文19篇，其中在IEEE Transactions on Multimedia （TMM）期刊上发表的关于手势识别的论文进入其学术领域中最优秀的ESI（1%）高被引用论文之列（0.38%）。谷歌学术显示现在的h指数为10，总引用数627。现担任包括IEEE Trans. Multimedia, IEEE Trans. Cybernetics, IEEE Trans. Human Machine System, IEEE Trans. Systems, Man, and Cybernetics: Systems等多种学术刊物的审稿人。一项关于 “三维场景的实时处理和交互系统”的项目入围首届中国创新挑战赛（西安赛区）的决赛，并获得优秀奖。获得2018-2019宁波市优秀论文三等奖。\n目前主持国家自然科学基金青年项目，浙江省自然科学基金青年项目，浙江省自然科学基金探索项目，国家重点实验室开放课题，浙江省教育厅科研项目和市自然科学基金项目各一项。曾参与多项香港政府创新及科技基金项目、香港政府优配研究金项目和国家重大科学工程项目。\n指导学生获得2016、2018、2021届校级优秀毕业设计(论文)，2020年宁波市计算机优秀学生论文，以及全国研究生数学建模竞赛和浙江省大学生机器人竞赛二等奖、三等奖等；2015-2016学年获学院优秀班主任称号。\n","date":1637366400,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1637366400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://cwang-nbu.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"王翀，博士，副教授，现任 宁波大学 信息科学与工程学院 计算机科学","tags":null,"title":"王 翀","type":"authors"},{"authors":["2016_XD_Wan"],"categories":null,"content":"现在网易工作\n毕业设计: 多深度相机的动态三维场景重建\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"31e2a8c9adf563e786690cd5d2e75e66","permalink":"https://cwang-nbu.github.io/authors/2016_xd_wan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/2016_xd_wan/","section":"authors","summary":"现在网易工作 毕业设计: 多深度相机的动态三维场景重建","tags":null,"title":"万旭东","type":"authors"},{"authors":["2019_YX_Lu"],"categories":null,"content":"深圳大学 硕士研究生 在读\n毕业设计: 基于深度卷积网络的对特定AR形象的目标检测\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"05d980d3702509e3db7798c49d9517de","permalink":"https://cwang-nbu.github.io/authors/2019_yx_lu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/2019_yx_lu/","section":"authors","summary":"深圳大学 硕士研究生 在读 毕业设计: 基于深度卷积网络的对特定AR","tags":null,"title":"卢雨心","type":"authors"},{"authors":["2019_DH_Li"],"categories":null,"content":"西北大学 硕士研究生 在读\n毕业设计: 猫狗大战\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"df75c9f3c265d80c0938d183f45184f8","permalink":"https://cwang-nbu.github.io/authors/2019_dh_li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/2019_dh_li/","section":"authors","summary":"西北大学 硕士研究生 在读 毕业设计: 猫狗大战","tags":null,"title":"李冬荟","type":"authors"},{"authors":null,"categories":null,"content":"In this tutorial, I’ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, …","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"https://cwang-nbu.github.io/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I’ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices …","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"https://cwang-nbu.github.io/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":null,"categories":null,"content":"国家自然科学基金 青年项目\n手势交互是智能人机交互技术的研究热点之一，在多媒体交互、人车交互、行为分析等领域有着广泛应用前景。手势识别的准确性、实时性、抗干扰性等是手势交互系统的关键。\n本研究在充分研究深度和彩色图像联合处理和超像素图特征表示的基础上，提出了新的高性能静态手势识别算法，并结合三维深度卷积网络和时域卷积网络，提出了一种动态手势识别模型。本研究利用深度和骨架信息，快速且准确地提取手势图像块。并利用手掌面的法向量估计有效地了抑制旋转、形变等干扰。在此基础上，基于概率的局部多项式回归算法能高质量地修复深度图像，大大减少噪声对识别准确性的影响。本研究在结合了超像素分割、手势结构和EMD距离的基础上，提出了一种新的手势表示形式（超像素图）和一种新的距离度量标准（基于标准化超像素图的EMD距离）。基于此研究结果，所构建的静态手势识别算法，对训练数据依赖度低，同时识别精度高。\n在五个公开数据集上，与多个最先进算法进行比较，均取得了最好的识别准确率（99.7%, 99.4%, 97.9%, 96.6%, 97.4%）。同时，本研究设计了一种用三维卷积网络提取动态手势的短时空时特征的网络结构，结合时间卷积网络和时域注意力机制，提出了新的短时时间卷积网络模型用于动态手势识别。提出的模型能够很好的分析动态手势的时域信息，在VIVA和NVGesture这两个公开数据集上，针对不同类型的数据，取得了与最新算法相当或更高的识别精度(91.54%, 86.10%, 86.21%, 86.93%)。\n在提出的识别算法的基础上，本研究通过三维打印制造了机械手，并实现了两个实际应用1) 五指机器人灵巧手的镜像操控和2) 三维场景漫游。本研究对静态和动态手势识别从方法到系统搭建进行细致而全面的研究，对基于手势的人机交互系统所需要解决的问题进行了理论探索，有着重要的科学意义和应用前景。\n","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1483228800,"objectID":"c3d40a696d2fb8997ffc09db49c5de23","permalink":"https://cwang-nbu.github.io/project/nsfc2017-project/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/project/nsfc2017-project/","section":"project","summary":"本项目主要围绕手势数据预处理，静、动态手势特征提取和距离度量，实时手势交互应用这三个方面展开研究。","tags":["手势识别","超像素","超体素"],"title":"基于超像/体素图的手势识别方法","type":"project"},{"authors":null,"categories":null,"content":"浙江省自然科学基金 探索项目\n人机智能交互技术是时下各国政府和企业关注的重点，手势交互以其非接触操作的优势成为自然人机交互的研究热点之一。虽然近年来这一领域已取得了可观的进展，但手势的多样性和环境的复杂性仍然是研究和应用中面临的最大挑战。如何保证手势检测和手势识别的高精度、实时性和稳健性，已成为手势交互系统向应用推进的关键与瓶颈。\n因此，本项目将主要围绕手势检测和预处理，高精度手势分类算法这两个方面展开研究。基于深度相机，联合利用深度、颜色和红外信息对手势进行预处理，结合深度学习中目标检测模型，解决手势检测的稳健性问题；在时域引入注意力机制，利用三维卷积网络和时间卷积网络分别提取短时和长时动态手势特征，进而构建高精度的动态手势识别算法。本课题对动态手势识别方法进行了细致而全面的研究，为基于手势识别的人机交互系统在实际应用中迫切需要解决的问题提供了良好的理论支持，有着重要的科学意义和应用前景。\n","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1577836800,"objectID":"72a57d29398907dbd10df459eace2a39","permalink":"https://cwang-nbu.github.io/project/zjnsfc2020-project/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/project/zjnsfc2020-project/","section":"project","summary":"在时域引入注意力机制，利用三维卷积网络和时间卷积网络分别提取短时和长时动态手势特征，进而构建高精度的动态手势识别算法。","tags":["手势识别","时域卷积网络","三维卷积网络"],"title":"基于长短时域特征的动态手势识别方法","type":"project"},{"authors":null,"categories":null,"content":"浙江省自然科学基金 青年项目\n智能人机交互技术是时下各国政府和企业关注的重点，手势交互以其非接触操作的优 势成为研究热点之一。虽然近年来已取得一定的进展，但手势的多样性和环境的复杂性仍 然是研究和应用中面临的最大挑战。如何保证手势识别的准确性和实时性，已成为手势交互系统发展的关键与瓶颈。\n本研究在充分研究深度和彩色图像联合处理和超像素图特征表示的基础上，提出了一种新的、更加高性能的手势识别方法。本研究利用深度和骨架信息，快速且准确地提取手势图像块。并利用手掌面的法向量估计有效地了抑制旋转、形变等干扰。在此基础上，基于概率的局部多项式回归算法能高质量地修复深度图像，大大减少噪声对识别准确性的影响。本研究在结合了超像素分割、手势结构和EMD距离的基础上，提出了一种新的手势表示形式（超像素图）和一种新的距离度量标准（基于标准化超像素图的EMD距离）。基于此研究结果，所构建的手势识别算法，对训练数据依赖度低，同时识别精度高。在五个公开数据集上，与多个最新算法进行比较，取得了最好的识别准确率。更进一步的是，本研究利用GPU加速等方法，实现了研究的手势识别算法的实时化，最终实现了两个实际应用的演示分别是1) 五指机器人灵巧手的镜像操控和2) 三维场景漫游。本研究对手势识别方法进行细致而全面的研究，为基于手势的人机交互系统迫切需要解决 的问题提供理论支持，有着重要的科学意义和应用前景。\n","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1577836800,"objectID":"ce2689867ad18c83b141d0c2d6c540e5","permalink":"https://cwang-nbu.github.io/project/zjnsfc2016-project/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/project/zjnsfc2016-project/","section":"project","summary":"本研究在结合了超像素分割、手势结构和EMD距离的基础上，提出了一种新的手势表示形式和新的距离度量标准。","tags":["手势识别","超像素图","EMD距离"],"title":"基于超像素图EMD距离的手势识别及交互","type":"project"},{"authors":null,"categories":null,"content":"参与“北大深研院”和“深国科”校企合作项目\n随着经济社会的发展，汽车越来越普及，人们对于汽车的安全性要求也随之提高。汽车的安全性能提高不仅有利于驾驶员、乘客，也间接地对城市的道路交通状况有帮助。2-4路360度环视技术作为驾驶员驾车的辅助技术，给驾驶员一个新的视角：高空鸟瞰。通过高空鸟瞰的视角对于车辆四周360度环绕有非常清楚的认识，进而可以帮助驾驶员更好的分析、判断路况，特别是在倒车入库、低速行车过程中，360度环视技术可以显著提高行车安全性。\n通过安装在车身上的2-4个鱼眼摄像头，获得前后左右2-4个方向的图片。对于这2-4个图片进行鱼眼矫正、视角转换、图像色差处理、环视合成这2-4个步骤，得到最终的环视图并显示在驾驶员的屏幕上，辅助驾驶员判断车子附近的情况。目前以opencv等开源的图像库为基础，构建、优化我们的360度环视系统。在此技术基础上，可以引入更多技术：图像显示方面，把2D显示转换为3D；速度方面：通过GPU加速计算，使得每一秒处理的图片更多，使得环视系统更有实时性，这些是我们增强360度环视系统的方向。\n","date":1477958400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1477958400,"objectID":"bc02a9dc6fca1590fcbded08adf1ee8b","permalink":"https://cwang-nbu.github.io/project/adas-project/","publishdate":"2016-11-01T00:00:00Z","relpermalink":"/project/adas-project/","section":"project","summary":"2-4路视频全景拼接系统基于时序图像利用之前时刻的图像填充当前相机的盲区部分，从而实现360度环视的效果。","tags":["全景拼接","相机矫正","RANSAC"],"title":"2-4路360度环视技术","type":"project"},{"authors":[],"categories":null,"content":"Slides can be added in a few ways:\n Create slides using Academic’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"https://cwang-nbu.github.io/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":["王 翀"],"categories":["研究生"],"content":"一年一度的聚餐\n今年学生规模大幅增长+全员到齐，去年的自助餐已经吃不起了\n今年吃高性价比日料，一个包厢已经坐不下了。\n明年吃什么更难定了。\n","date":1637366400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1637366400,"objectID":"b74fbfff3fbbef83ee4586696110850a","permalink":"https://cwang-nbu.github.io/post/202111_%E7%A0%94%E7%A9%B6%E7%94%9F%E8%81%9A%E9%A4%90/","publishdate":"2021-11-20T00:00:00Z","relpermalink":"/post/202111_%E7%A0%94%E7%A9%B6%E7%94%9F%E8%81%9A%E9%A4%90/","section":"post","summary":"稻炭吃日料","tags":["研究生","聚餐"],"title":"2021年研究生聚餐","type":"post"},{"authors":["王 翀"],"categories":["论文"],"content":"全团队学术活动\n时隔2年，有一次全实验室参加VALSE 2021，声势浩大。\n不能出国参加国际会议的现在，对所有同学都是一次很好的经验，希望对每个人都有帮助和启发。\n白天听大佬讲座，晚上撸烤串，睡五星酒店。\n2022年在青岛可以再来一次。\n","date":1633824000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1633824000,"objectID":"7d4a839fee093c2b4b389abc19d4c092","permalink":"https://cwang-nbu.github.io/post/20211010_valse2021/","publishdate":"2021-10-10T00:00:00Z","relpermalink":"/post/20211010_valse2021/","section":"post","summary":"全队参加VALSE 2021","tags":["会议","研究生","VALSE"],"title":"全实验室参加视觉与学习青年学者研讨会","type":"post"},{"authors":["郁盛浩","王 翀","毛乔梅","Yuqi Li","Jiafei Wu"],"categories":null,"content":"","date":1633046400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1633046400,"objectID":"8c367837c9d739955872ee9f6c94292c","permalink":"https://cwang-nbu.github.io/publication/journal-article-spl2021/","publishdate":"2021-10-01T00:00:00Z","relpermalink":"/publication/journal-article-spl2021/","section":"publication","summary":"In this work, a cross-epoch learning (XEL) strategy associated with a hard instance bank (HIB) is proposed to introduce additional information from previous training epochs.","tags":["异常行为检测","半监督学习","卷积神经网络"],"title":"Cross-Epoch Learning for Weakly Supervised Anomaly Detection in Surveillance Videos","type":"publication"},{"authors":["王 翀","郁盛浩"],"categories":["论文"],"content":"恭喜郁盛浩\n论文 “Cross-Epoch Learning for Weakly Supervised Anomaly Detection in Surveillance Videos” 被 ISCAS 2020 和 IEEE Signal Processing Letters 接收。\n可以安心整理毕业论文，找个996的好工作了。\n","date":1633046400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1633046400,"objectID":"f980ab6607482128dc07e9259d933366","permalink":"https://cwang-nbu.github.io/post/202110_%E9%83%81%E7%9B%9B%E6%B5%A9spl/","publishdate":"2021-10-01T00:00:00Z","relpermalink":"/post/202110_%E9%83%81%E7%9B%9B%E6%B5%A9spl/","section":"post","summary":"不负辛苦，终于修成正果","tags":["期刊","SPL"],"title":"祝贺郁盛浩论文被IEEE Signal Processing Letters接收","type":"post"},{"authors":["Jiafei Wu","Genjie Li","王 翀","Huakai Liu","Shuai Zhang","Guangcheng Zhang"],"categories":null,"content":"","date":1625443200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1625443200,"objectID":"89cc07a457c6a6c94b8dbd1bdeb9fb2e","permalink":"https://cwang-nbu.github.io/publication/conference-paper-icmew2021/","publishdate":"2021-06-30T00:00:00Z","relpermalink":"/publication/conference-paper-icmew2021/","section":"publication","summary":"In this paper, we propose an extended GIF (EGIF) by incorporating a novel edge-aware factor into the regularization term.","tags":["Guided image filtering","Entropy-variance domain","Contrast enhancement"],"title":"Extended Guided Image Filtering For Contrast Enhancement","type":"publication"},{"authors":["王 翀","毛乔梅"],"categories":["研究生","毕业"],"content":"欢迎新同学加入团队\n首届毕业生请吃饭，大家表情都有点呆滞 \u0026gt;_\u0026lt;\n毛乔梅在整个研究生期间对基础理论知识和相关编程能力的学习和掌握均较好，在研究过程中能积极探索新的思路和方案，思维活跃沟通顺畅，自主性和条理性较强，在实验和论文遇到困难时也能沉下心来耐心攻克。因此，其在研究生三年内获得多项荣誉和奖学金，在同期学生中也是佼佼者。在京东算法部门进行实习后，对自我的要求更上了一层楼，希望正式工作后能继续够发挥自己所长，不断学习进取，取得更好的成果。\n","date":1624406400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1624838400,"objectID":"ebd1e9be669d9dccbc1af0bc5a9e77fa","permalink":"https://cwang-nbu.github.io/post/202106_%E6%AF%95%E4%B8%9A%E9%A4%90/","publishdate":"2021-06-23T00:00:00Z","relpermalink":"/post/202106_%E6%AF%95%E4%B8%9A%E9%A4%90/","section":"post","summary":"祝生活+事业顺利","tags":["毕业餐","研究生"],"title":"2021年毕业生聚餐","type":"post"},{"authors":["王 翀"],"categories":["研究生","招生"],"content":"欢迎新同学加入团队\n今年名额2+1个：\n 郑瑜杰 陶晨晨 戴鑫淼  ","date":1616457600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1616889600,"objectID":"a441ce33a822f25349065cb4daf2f6d4","permalink":"https://cwang-nbu.github.io/post/202103_%E7%A0%94%E7%A9%B6%E7%94%9F%E6%8B%9B%E7%94%9F/","publishdate":"2021-03-23T00:00:00Z","relpermalink":"/post/202103_%E7%A0%94%E7%A9%B6%E7%94%9F%E6%8B%9B%E7%94%9F/","section":"post","summary":"欢迎第四届新同学","tags":["新生","研究生"],"title":"2021年研究生招生","type":"post"},{"authors":["陈魏魏","王 翀","Linlin Gao"],"categories":null,"content":"","date":1616284800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1616284800,"objectID":"001afab30bdc74f49937343486e9daf3","permalink":"https://cwang-nbu.github.io/publication/conference-paper-icassp2021/","publishdate":"2021-03-21T00:00:00Z","relpermalink":"/publication/conference-paper-icassp2021/","section":"publication","summary":"In this paper, a new reweighted dynamic group convolution (RDGC) structure, including a reweighted pruning module and a survival loss, is proposed in this work for more precise channel pruning.","tags":["分组卷积","深度学习","网络轻量化"],"title":"Reweighted Dynamic Group Convolution","type":"publication"},{"authors":["王 翀","陈魏魏"],"categories":["论文"],"content":"恭喜陈魏魏\n论文 “Reweighted Dynamic Group Convolution” 被 ICASSP 2021 （CCF-B类）接收\n做图像的蹭一下NLP领域的顶会\n","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1616371200,"objectID":"44c4d47d861ba0de5054f04bb57b2878","permalink":"https://cwang-nbu.github.io/post/202103_%E9%99%88%E9%AD%8F%E9%AD%8Ficassp/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/post/202103_%E9%99%88%E9%AD%8F%E9%AD%8Ficassp/","section":"post","summary":"ICASSP 2021 接收","tags":["会议","ICASSP"],"title":"祝贺陈魏魏ICASSP 2021论文接收","type":"post"},{"authors":["王 翀"],"categories":["研究生"],"content":"一年一度的聚餐\n去年的烤肉店已经倒闭了，今年在华侨豪生吃自助，可惜毛乔梅和陈魏魏缺席。\n以后要好好拍一张照片。\n","date":1605916800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1605916800,"objectID":"6e95db99df1700b3673be567e69daac8","permalink":"https://cwang-nbu.github.io/post/202011_%E7%A0%94%E7%A9%B6%E7%94%9F%E8%81%9A%E9%A4%90/","publishdate":"2020-11-21T00:00:00Z","relpermalink":"/post/202011_%E7%A0%94%E7%A9%B6%E7%94%9F%E8%81%9A%E9%A4%90/","section":"post","summary":"华侨豪生吃自助","tags":["研究生","聚餐"],"title":"2020年研究生聚餐","type":"post"},{"authors":["王 翀"],"categories":["研究生"],"content":"恭喜毛乔梅获得国家奖学金\n其他同学也要加油!\nps. 中央财政出资设立研究生国家奖学金，用于奖励普通高等学校(“国家奖学金”荣誉证书“国家奖学金”荣誉证书(2张)以下简称高等学校)中表现优异的全日制研究生。研究生国家奖学金每年奖励4.5万名在读研究生。其中，博士研究生1万名，硕士研究生为3.5万名。博士研究生国家奖学金奖励标准定为每生每年3万元；硕士研究生国家奖学金奖励标准定为每生每年2万元\n","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1604188800,"objectID":"f19febb5bf5ed1c4ca7c4213cc04945a","permalink":"https://cwang-nbu.github.io/post/202010_%E5%A5%96%E5%AD%A6%E9%87%91/","publishdate":"2020-11-01T00:00:00Z","relpermalink":"/post/202010_%E5%A5%96%E5%AD%A6%E9%87%91/","section":"post","summary":"毛乔梅获得国家奖学金","tags":["奖学金","研究生"],"title":"2020年研究生奖学金","type":"post"},{"authors":["王 翀","Wei Liu","Mengdi Jiang"],"categories":null,"content":"","date":1600646400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1600646400,"objectID":"97460a06eda0554aac7a164cc5f790e3","permalink":"https://cwang-nbu.github.io/publication/conference-paper-mlsp2020/","publishdate":"2020-09-21T00:00:00Z","relpermalink":"/publication/conference-paper-mlsp2020/","section":"publication","summary":"In this paper, a convolutional neural network (CNNs) based approach for target direction finding with the thinned coprime array (TCA) as an example is proposed.","tags":["卷积神经网络","Thinned Coprime A","DOA"],"title":"A Unified Approach for Target Direction Finding based on Convolutional Neural Networks","type":"publication"},{"authors":["王 翀"],"categories":["研究生","招生"],"content":"欢迎新同学加入团队\n霍铮，陈松，龚益玲\n","date":1589760000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1589760000,"objectID":"517ceb9f64a45cc05c58045b7621cf3e","permalink":"https://cwang-nbu.github.io/post/202005_%E7%A0%94%E7%A9%B6%E7%94%9F%E6%8B%9B%E7%94%9F/","publishdate":"2020-05-18T00:00:00Z","relpermalink":"/post/202005_%E7%A0%94%E7%A9%B6%E7%94%9F%E6%8B%9B%E7%94%9F/","section":"post","summary":"新的同学加入了","tags":["新生","研究生"],"title":"2020年研究生招生","type":"post"},{"authors":["毛乔梅","王 翀","郁盛浩","Ye Zheng","Yuqi Li"],"categories":null,"content":"提取码: UxQp\n","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1588291200,"objectID":"41b902e56d5961532be60b68b60cfc70","permalink":"https://cwang-nbu.github.io/publication/journal-article-acs-zsd/","publishdate":"2020-05-01T00:00:00Z","relpermalink":"/publication/journal-article-acs-zsd/","section":"publication","summary":"This brief proposes a new algorithm using attributes based category similarity for accurate alignment between visual features and semantic concepts.","tags":["目标检测","零样本学习","属性表","卷积神经网络"],"title":"Zero-Shot Object Detection with Attributes based Category Similarity","type":"publication"},{"authors":["王 翀","毛乔梅"],"categories":["论文"],"content":"恭喜毛乔梅\n论文 “Zero-Shot Object Detection with Attributes based Category Similarity” 被 ISCAS 2020 和 IEEE Transactions on Circuits and Systems II: Express Briefs 接收。\n可以去西班牙旅游了！\n可惜疫情推迟到10月了，希望能成行。\n改成线上会议了，等着录视频。\n","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1585526400,"objectID":"3c2c663dbcc8a10023b9cd28a8b08640","permalink":"https://cwang-nbu.github.io/post/202003_%E6%AF%9B%E4%B9%94%E6%A2%85tcasii/","publishdate":"2020-03-01T00:00:00Z","relpermalink":"/post/202003_%E6%AF%9B%E4%B9%94%E6%A2%85tcasii/","section":"post","summary":"ISCAS 2020 接收，并被推荐TCAS-II，也接收了","tags":["会议","期刊","TCAS-II","ISCAS"],"title":"祝贺毛乔梅ISCAS接收并推荐TCAS-II","type":"post"},{"authors":["Yi Zhang","王 翀","Ye Zheng","Jieyu Zhao","Yuqi Li","Xijiong Xie"],"categories":null,"content":"","date":1575676800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1575676800,"objectID":"58a76a8c8137332a03f4883910b4a648","permalink":"https://cwang-nbu.github.io/publication/preprint-2019/","publishdate":"2019-12-01T00:00:00Z","relpermalink":"/publication/preprint-2019/","section":"publication","summary":"In this paper, we present a multimodal gesture recognition method based on 3D densely convolutional networks (3D-DenseNets) and improved temporal convolutional networks (TCNs).","tags":["手势识别","时间卷积网络(TCN)","DenseNet"],"title":"Short-Term Temporal Convolutional Networks for Dynamic Hand Gesture Recognition","type":"publication"},{"authors":["王 翀"],"categories":["学术交流"],"content":"简单记录下在谢菲尔德科研生活\n周期一：2019-09-02 至 2019-12-02\n 本阶段学习和研修基本情况  本人于2019年9月2日抵达英国伦敦，9月3日抵达留学单位（谢菲尔德大学，University of Sheffield），并在当日中午与国外导师（刘伟博士）见面进行了初步的讨论，当日下午和电子电机工程系的系主任秘书Ms Kim Brechin见面，完成了相关报道手续。之后，在9月中上旬，完成了租房、警局注册、设备安置等生活和研修相关的安顿工作，并向曼彻斯特总领馆教育处报道。\n在研修过程中，与国外导师（刘伟博士）和其课题组内的蒋萌迪博士多次见面，讨论了合作研究的方向和大体思路，打算将我正在进行的深度学习相关的内容和刘伟博士研究组的阵列信号处理相结合，既能利用深度学习提高阵列信号处理的性能，也能将阵列信号相关内容应用于我在研究的图像识别领域。在9月至11月，主要进行了以下研修内容，\na)\t对阵列信号处理相关的内容进行了了解和学习，部分内容已在研究生阶段的研究中有所涉及，但仍有不少概念和数学上的知识需要补充。\nb)\t仔细研读了与我们讨论决定的研究方向相近的发表于IEEE Transactions on Antennas and Propagation的论文”Direction-of-Arrival Estimation Based on Deep Neural Networks With Robustness to Array Imperfections”，经过讨论决定参考其框架进行后续研究。\nc)\t完成了一个初步的简单神经网络的构建，包括一个单输入多输出的AutoEncoder用于DOA输入信号的分离，正在生成和调整数据以对网络进行学习和测试。\n发表的论文及专利等科研成果  由于刚抵达英国，合作相关的研究刚处于起步阶段，还没有发表的论文等科研成果。在9月至11月期间，继续之前在国内的研究，指导学生撰写和投稿了2篇会议论文和1篇期刊论文，具体如下：\na)\t“Action Recognition with Temporal Selection”投稿至CCF C类会议ISCAS 2020。\nb)\t“Zero-shot Object Detection with Attributes based Category Similarity” 投稿至CCF C类会议ISCAS 2020。\nc)\t“Partial Person Re-identification with Pose-guided Alignment ”投稿至SCI期刊Applied Intelligence。\n参加学术活动情况  由于有不少时间用于生活上的安顿，了解学校设施和规章制度，以及研究计划的讨论和安排，这一研修周期中并没有参加其他的学术活动。不过计划参加12月12日的MAPP Lecture，由来自剑桥大学的Dr Phillip Stanley-Marbell作题为Augmenting Raw Materials with Sensing and Computation的报告。\n接下来的研修计划  接下来的研究主要针对讨论确定的研究路线进行展开，预计完成以下工作，\na)\t实现1.b)中所提的TAP论文中的整个网络并进行改进，即使用VAE替代AutoEncoder，并将后半的网络用CNN替换论文中使用的MLP，同时完成DOA数据的生成和训练。\nb)\t将复数和四元数的计算结合进CNN中，参考Github上已经有的部分代码和实现(github.com/wavefrontshaping/complexPyTorch 等)，用于阵列信号的处理。\nc)\t在2020年2月，与国外导师（刘伟博士）和其课题组内的蒋萌迪博士等合作撰写会议论文并投稿。\n周期二：2019-12-02 至 2020-03-02\n 本阶段学习和研修基本情况  在本阶段研修过程中，着重进行了将深度学习应用于阵列信号方向估计的问题。与国外导师（刘伟博士）和其课题组内的蒋萌迪博士多次见面，讨论了具体的数据形式、阵列选择、深度神经网络模型等。主要工作集中在生成各类数据、设计和搭建神经网络框架，训练网络模型，并且验证实验结果。在12月至2月，主要进行了以下研修内容，\na)\t生成了大量Uniform Linear Array（ULA）和CoPrime Array的阵列信号训练数据。包括\n  0至90度的1-12个随机信号源的10个阵元的ULA信号的100万组covariance matrix训练数据，1万组测试数据，\n  -90至90度的1-16个随机信号源的12个阵元的CoPrime Array信号的100万，200万和1000万的covariance matrix训练数据（512个采样）各2组，10万的验证数据和测试数据各2组，\n  -60至60度的1-16个随机信号源的12个阵元的Co-Prime Array信号的200万和500万的covariance matrix以及原始信号（12，16，24个采样各一组）的训练数据各2组，10万的验证数据和测试数据各2组。\n  b)\t针对生成的训练、验证和测试数据，我设计了传统的30层CNN进行初步测试，得到一些初步结果。然后在ResNeXt-50和ResNeXt-101的基础上，对网络结果进行修改，并使用a)中生成的各个训练数据集进行训练，利用验证数据集确定训练论述，使用测试数据集进行测试。取得了各个实验设置下的结果，最好的512采样的数据集在限制入射角度为-60至60度时，能够达到99.9%以上的准确度。\nc)\t根据现有数据集的结果，发现样本数对结果影响较大，covariance matrix结果要好于原始信号。同时，在检查训练过程中的validation loss时，发现有过拟合的现象。正在对网路、数据、训练过程等进行调整。\n发表的论文及专利等科研成果  在12月至2月期间，继续在国内的研究，指导学生撰写和投稿了2篇会议论文和1篇期刊论文，具体如下：\na)\t“Zero-shot Object Detection with Attributes based Category Similarity” 被CCF C类会议ISCAS 2020作为Oral接收。\nb)\t作为优秀ISCAS 2020论文收到邀请，修改和扩展了“Zero-shot Object Detection with Attributes based Category Similarity”这篇论文，投稿至TCAS-II Special Issue。\nc)\t“Action Recognition with Temporal Selection”被ISCAS 2020拒稿，经过大幅度修改后投稿至CCF C类会议ICIP 2020。\n参加学术活动情况  在这一研修周期中参加了以下学术活动，\na)\t参加了University of Bristol 的Dr Ejay Nsugbe 的关于\u0026#34;Upper Limb Prosthesis for Trans-humeral Amputees\u0026#34;的讲座，\nb)\t参加了Dr Kevin Li Sun的关于 “Deep Learning for the Robot to Think Fast”，\nc)\t参加了Dr Nicolas Herzig的关于\u0026#34;Compliance in Robotics: A Journey through Human-Robot Environment Interactions\u0026#34;的讲座，\nd)\t参加了Engineering You’re Hired (EYH)的活动，和当地学生进行交流。\n接下来的研修计划  接下来的研究主要针对讨论确定的研究路线进行展开，预计完成以下工作，\na)\t通过更大的数据集、不同网路结构等，对模型的过拟合进行处理。\nb)\t对结果进行整理，与国外导师（刘伟博士）和其课题组内的蒋萌迪博士等合作撰写论文并投稿。\n","date":1567641600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1587081600,"objectID":"d39892f80891c2a56d15deef921155c1","permalink":"https://cwang-nbu.github.io/post/201909_uklife/","publishdate":"2019-09-05T00:00:00Z","relpermalink":"/post/201909_uklife/","section":"post","summary":"希望一切顺利.","tags":["访学","英国"],"title":"开始在英国谢菲尔德大学访问的一年","type":"post"},{"authors":["王 翀"],"categories":["学术交流"],"content":"简单记录下在谢菲尔德科研生活\n周期一：2019-09-02 至 2019-12-02\n 本阶段学习和研修基本情况  本人于2019年9月2日抵达英国伦敦，9月3日抵达留学单位（谢菲尔德大学，University of Sheffield），并在当日中午与国外导师（刘伟博士）见面进行了初步的讨论，当日下午和电子电机工程系的系主任秘书Ms Kim Brechin见面，完成了相关报道手续。之后，在9月中上旬，完成了租房、警局注册、设备安置等生活和研修相关的安顿工作，并向曼彻斯特总领馆教育处报道。\n在研修过程中，与国外导师（刘伟博士）和其课题组内的蒋萌迪博士多次见面，讨论了合作研究的方向和大体思路，打算将我正在进行的深度学习相关的内容和刘伟博士研究组的阵列信号处理相结合，既能利用深度学习提高阵列信号处理的性能，也能将阵列信号相关内容应用于我在研究的图像识别领域。在9月至11月，主要进行了以下研修内容，\na)\t对阵列信号处理相关的内容进行了了解和学习，部分内容已在研究生阶段的研究中有所涉及，但仍有不少概念和数学上的知识需要补充。\nb)\t仔细研读了与我们讨论决定的研究方向相近的发表于IEEE Transactions on Antennas and Propagation的论文”Direction-of-Arrival Estimation Based on Deep Neural Networks With Robustness to Array Imperfections”，经过讨论决定参考其框架进行后续研究。\nc)\t完成了一个初步的简单神经网络的构建，包括一个单输入多输出的AutoEncoder用于DOA输入信号的分离，正在生成和调整数据以对网络进行学习和测试。\n发表的论文及专利等科研成果  由于刚抵达英国，合作相关的研究刚处于起步阶段，还没有发表的论文等科研成果。在9月至11月期间，继续之前在国内的研究，指导学生撰写和投稿了2篇会议论文和1篇期刊论文，具体如下：\na)\t“Action Recognition with Temporal Selection”投稿至CCF C类会议ISCAS 2020。\nb)\t“Zero-shot Object Detection with Attributes based Category Similarity” 投稿至CCF C类会议ISCAS 2020。\nc)\t“Partial Person Re-identification with Pose-guided Alignment ”投稿至SCI期刊Applied Intelligence。\n参加学术活动情况  由于有不少时间用于生活上的安顿，了解学校设施和规章制度，以及研究计划的讨论和安排，这一研修周期中并没有参加其他的学术活动。不过计划参加12月12日的MAPP Lecture，由来自剑桥大学的Dr Phillip Stanley-Marbell作题为Augmenting Raw Materials with Sensing and Computation的报告。\n接下来的研修计划  接下来的研究主要针对讨论确定的研究路线进行展开，预计完成以下工作，\na)\t实现1.b)中所提的TAP论文中的整个网络并进行改进，即使用VAE替代AutoEncoder，并将后半的网络用CNN替换论文中使用的MLP，同时完成DOA数据的生成和训练。\nb)\t将复数和四元数的计算结合进CNN中，参考Github上已经有的部分代码和实现(github.com/wavefrontshaping/complexPyTorch 等)，用于阵列信号的处理。\nc)\t在2020年2月，与国外导师（刘伟博士）和其课题组内的蒋萌迪博士等合作撰写会议论文并投稿。\n周期二：2019-12-02 至 2020-03-02\n 本阶段学习和研修基本情况  在本阶段研修过程中，着重进行了将深度学习应用于阵列信号方向估计的问题。与国外导师（刘伟博士）和其课题组内的蒋萌迪博士多次见面，讨论了具体的数据形式、阵列选择、深度神经网络模型等。主要工作集中在生成各类数据、设计和搭建神经网络框架，训练网络模型，并且验证实验结果。在12月至2月，主要进行了以下研修内容，\na)\t生成了大量Uniform Linear Array（ULA）和CoPrime Array的阵列信号训练数据。包括\n  0至90度的1-12个随机信号源的10个阵元的ULA信号的100万组covariance matrix训练数据，1万组测试数据，\n  -90至90度的1-16个随机信号源的12个阵元的CoPrime Array信号的100万，200万和1000万的covariance matrix训练数据（512个采样）各2组，10万的验证数据和测试数据各2组，\n  -60至60度的1-16个随机信号源的12个阵元的Co-Prime Array信号的200万和500万的covariance matrix以及原始信号（12，16，24个采样各一组）的训练数据各2组，10万的验证数据和测试数据各2组。\n  b)\t针对生成的训练、验证和测试数据，我设计了传统的30层CNN进行初步测试，得到一些初步结果。然后在ResNeXt-50和ResNeXt-101的基础上，对网络结果进行修改，并使用a)中生成的各个训练数据集进行训练，利用验证数据集确定训练论述，使用测试数据集进行测试。取得了各个实验设置下的结果，最好的512采样的数据集在限制入射角度为-60至60度时，能够达到99.9%以上的准确度。\nc)\t根据现有数据集的结果，发现样本数对结果影响较大，covariance matrix结果要好于原始信号。同时，在检查训练过程中的validation loss时，发现有过拟合的现象。正在对网路、数据、训练过程等进行调整。\n发表的论文及专利等科研成果  在12月至2月期间，继续在国内的研究，指导学生撰写和投稿了2篇会议论文和1篇期刊论文，具体如下：\na)\t“Zero-shot Object Detection with Attributes based Category Similarity” 被CCF C类会议ISCAS 2020作为Oral接收。\nb)\t作为优秀ISCAS 2020论文收到邀请，修改和扩展了“Zero-shot Object Detection with Attributes based Category Similarity”这篇论文，投稿至TCAS-II Special Issue。\nc)\t“Action Recognition with Temporal Selection”被ISCAS 2020拒稿，经过大幅度修改后投稿至CCF C类会议ICIP 2020。\n参加学术活动情况  在这一研修周期中参加了以下学术活动，\na)\t参加了University of Bristol 的Dr Ejay Nsugbe 的关于\u0026#34;Upper Limb Prosthesis for Trans-humeral Amputees\u0026#34;的讲座，\nb)\t参加了Dr Kevin Li Sun的关于 “Deep Learning for the Robot to Think Fast”，\nc)\t参加了Dr Nicolas Herzig的关于\u0026#34;Compliance in Robotics: A Journey through Human-Robot Environment Interactions\u0026#34;的讲座，\nd)\t参加了Engineering You’re Hired (EYH)的活动，和当地学生进行交流。\n接下来的研修计划  接下来的研究主要针对讨论确定的研究路线进行展开，预计完成以下工作，\na)\t通过更大的数据集、不同网路结构等，对模型的过拟合进行处理。\nb)\t对结果进行整理，与国外导师（刘伟博士）和其课题组内的蒋萌迪博士等合作撰写论文并投稿。\n","date":1567641600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1587081600,"objectID":"39a3d8c6257e5c765e6c44d6b30bbf6c","permalink":"https://cwang-nbu.github.io/post/201909_%E8%8B%B1%E5%9B%BD%E8%AE%BF%E5%AD%A6/","publishdate":"2019-09-05T00:00:00Z","relpermalink":"/post/201909_%E8%8B%B1%E5%9B%BD%E8%AE%BF%E5%AD%A6/","section":"post","summary":"希望一切顺利.","tags":["访学","英国"],"title":"开始在英国谢菲尔德大学访问的一年","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34;\rif porridge == \u0026#34;blueberry\u0026#34;:\rprint(\u0026#34;Eating...\u0026#34;)\r  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}}\r{{% fragment %}} **Two** {{% /fragment %}}\r{{% fragment %}} Three {{% /fragment %}}\r Press Space to play!\nOne \rTwo \rThree \r A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}}\r- Only the speaker can read these notes\r- Press `S` key to view\r{{% /speaker_note %}}\r Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/img/boards.jpg\u0026#34; \u0026gt;}}\r{{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}}\r{{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}}\r  Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1,\r.reveal section h2,\r.reveal section h3 {\rcolor: navy;\r}\r  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://cwang-nbu.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Jiafei Wu","王 翀","Yongze Xu"],"categories":null,"content":"","date":1532304000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1532304000,"objectID":"9b75e2fa1fd33c570fe63f8678774621","permalink":"https://cwang-nbu.github.io/publication/conference-paper-icme2018/","publishdate":"2018-07-23T00:00:00Z","relpermalink":"/publication/conference-paper-icme2018/","section":"publication","summary":"In this paper, an improved guided filter (IGIF) is proposed by incorporating an adaptive structure aware constraint.","tags":["导向滤波","图像增强"],"title":"An Improved Guided Filtering Algorithm for Image Enhancement","type":"publication"},{"authors":["王 翀","S.C. Chan","Li Zhang","H.Y. Shum"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1514764800,"objectID":"4ca04a2e5d49a00248c41af3763f6b0d","permalink":"https://cwang-nbu.github.io/publication/journal-article-tvc2018/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/journal-article-tvc2018/","section":"publication","summary":"This paper presents a superpixel-based joint color–depth restoration approach for Kinect depth camera and study its application to view synthesis in IBR systems.","tags":["超像素","图像恢复","Kinect","深度相机","基于图像的渲染(IBR)"],"title":"Superpixel-based color-depth restoration and dynamic environment modeling for Kinect-assisted image-based rendering systems","type":"publication"},{"authors":["王 翀","Zhong Liu","Minfeng Zhu","Jieyu Zhao","S.C. Chan"],"categories":null,"content":"本论文获得2017-2018年度宁波市自然科学优秀论文奖 三等奖\n","date":1498867200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1498867200,"objectID":"8d7fef40d94e26f5d3e818a0e662952a","permalink":"https://cwang-nbu.github.io/publication/journal-article-csgemd/","publishdate":"2017-07-01T00:00:00Z","relpermalink":"/publication/journal-article-csgemd/","section":"publication","summary":"This paper presents a new hand gesture recognition system based on a novel canonical superpixel-graph earth mover’s distance (CSG-EMD) metric.","tags":["手势识别","EMD距离","标准型","超像素图"],"title":"A hand gesture recognition system based on canonical superpixel-graph","type":"publication"},{"authors":["王 翀","Zhong Liu","S.C. Chan"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1420070400,"objectID":"d0c9932f0a4aecc6c66273e8c0819de9","permalink":"https://cwang-nbu.github.io/publication/journal-article-spemd/","publishdate":"2014-11-01T00:00:00Z","relpermalink":"/publication/journal-article-spemd/","section":"publication","summary":"In this work, a novel distance metric, superpixel earth mover's distance (SP-EMD), is proposed to measure the dissimilarity between the hand gestures.","tags":["手势识别","EMD距离","超像素"],"title":"Superpixel-Based Hand Gesture Recognition With Kinect Depth Camera","type":"publication"},{"authors":["王 翀"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1333756800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1333756800,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"https://cwang-nbu.github.io/publication/preprint/","publishdate":"2012-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"}]