<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>个人主页</title>
    <link>https://cwang-nbu.github.io/</link>
      <atom:link href="https://cwang-nbu.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>个人主页</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language><lastBuildDate>Sat, 01 Jun 2030 13:00:00 +0000</lastBuildDate>
    <image>
      <url>https://cwang-nbu.github.io/media/logo_hufe68acc6cba5b92c41e00edcb22ec9fb_65773_300x300_fit_lanczos_3.png</url>
      <title>个人主页</title>
      <link>https://cwang-nbu.github.io/</link>
    </image>
    
    <item>
      <title>Example Page 1</title>
      <link>https://cwang-nbu.github.io/courses/example/example1/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://cwang-nbu.github.io/courses/example/example1/</guid>
      <description>&lt;p&gt;In this tutorial, I&amp;rsquo;ll share my top 10 tips for getting started with Academic:&lt;/p&gt;
&lt;h2 id=&#34;tip-1&#34;&gt;Tip 1&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
&lt;h2 id=&#34;tip-2&#34;&gt;Tip 2&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Example Page 2</title>
      <link>https://cwang-nbu.github.io/courses/example/example2/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://cwang-nbu.github.io/courses/example/example2/</guid>
      <description>&lt;p&gt;Here are some more tips for getting started with Academic:&lt;/p&gt;
&lt;h2 id=&#34;tip-3&#34;&gt;Tip 3&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
&lt;h2 id=&#34;tip-4&#34;&gt;Tip 4&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>基于超像/体素图的手势识别方法</title>
      <link>https://cwang-nbu.github.io/project/nsfc2017-project/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://cwang-nbu.github.io/project/nsfc2017-project/</guid>
      <description>&lt;p&gt;&lt;strong&gt;国家自然科学基金 青年项目&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;手势交互是智能人机交互技术的研究热点之一，在多媒体交互、人车交互、行为分析等领域有着广泛应用前景。手势识别的准确性、实时性、抗干扰性等是手势交互系统的关键。&lt;/p&gt;
&lt;p&gt;本研究在充分研究深度和彩色图像联合处理和超像素图特征表示的基础上，提出了新的高性能静态手势识别算法，并结合三维深度卷积网络和时域卷积网络，提出了一种动态手势识别模型。本研究利用深度和骨架信息，快速且准确地提取手势图像块。并利用手掌面的法向量估计有效地了抑制旋转、形变等干扰。在此基础上，基于概率的局部多项式回归算法能高质量地修复深度图像，大大减少噪声对识别准确性的影响。本研究在结合了超像素分割、手势结构和EMD距离的基础上，提出了一种新的手势表示形式（超像素图）和一种新的距离度量标准（基于标准化超像素图的EMD距离）。基于此研究结果，所构建的静态手势识别算法，对训练数据依赖度低，同时识别精度高。&lt;/p&gt;
&lt;p&gt;在五个公开数据集上，与多个最先进算法进行比较，均取得了最好的识别准确率（99.7%, 99.4%, 97.9%, 96.6%, 97.4%）。同时，本研究设计了一种用三维卷积网络提取动态手势的短时空时特征的网络结构，结合时间卷积网络和时域注意力机制，提出了新的短时时间卷积网络模型用于动态手势识别。提出的模型能够很好的分析动态手势的时域信息，在VIVA和NVGesture这两个公开数据集上，针对不同类型的数据，取得了与最新算法相当或更高的识别精度(91.54%, 86.10%, 86.21%, 86.93%)。&lt;/p&gt;
&lt;p&gt;在提出的识别算法的基础上，本研究通过三维打印制造了机械手，并实现了两个实际应用1) 五指机器人灵巧手的镜像操控和2) 三维场景漫游。本研究对静态和动态手势识别从方法到系统搭建进行细致而全面的研究，对基于手势的人机交互系统所需要解决的问题进行了理论探索，有着重要的科学意义和应用前景。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>基于长短时域特征的动态手势识别方法</title>
      <link>https://cwang-nbu.github.io/project/zjnsfc2020-project/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://cwang-nbu.github.io/project/zjnsfc2020-project/</guid>
      <description>&lt;p&gt;&lt;strong&gt;浙江省自然科学基金 探索项目&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;人机智能交互技术是时下各国政府和企业关注的重点，手势交互以其非接触操作的优势成为自然人机交互的研究热点之一。虽然近年来这一领域已取得了可观的进展，但手势的多样性和环境的复杂性仍然是研究和应用中面临的最大挑战。如何保证手势检测和手势识别的高精度、实时性和稳健性，已成为手势交互系统向应用推进的关键与瓶颈。&lt;/p&gt;
&lt;p&gt;因此，本项目将主要围绕手势检测和预处理，高精度手势分类算法这两个方面展开研究。基于深度相机，联合利用深度、颜色和红外信息对手势进行预处理，结合深度学习中目标检测模型，解决手势检测的稳健性问题；在时域引入注意力机制，利用三维卷积网络和时间卷积网络分别提取短时和长时动态手势特征，进而构建高精度的动态手势识别算法。本课题对动态手势识别方法进行了细致而全面的研究，为基于手势识别的人机交互系统在实际应用中迫切需要解决的问题提供了良好的理论支持，有着重要的科学意义和应用前景。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>基于超像素图EMD距离的手势识别及交互</title>
      <link>https://cwang-nbu.github.io/project/zjnsfc2016-project/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://cwang-nbu.github.io/project/zjnsfc2016-project/</guid>
      <description>&lt;p&gt;&lt;strong&gt;浙江省自然科学基金 青年项目&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;智能人机交互技术是时下各国政府和企业关注的重点，手势交互以其非接触操作的优 势成为研究热点之一。虽然近年来已取得一定的进展，但手势的多样性和环境的复杂性仍 然是研究和应用中面临的最大挑战。如何保证手势识别的准确性和实时性，已成为手势交互系统发展的关键与瓶颈。&lt;/p&gt;
&lt;p&gt;本研究在充分研究深度和彩色图像联合处理和超像素图特征表示的基础上，提出了一种新的、更加高性能的手势识别方法。本研究利用深度和骨架信息，快速且准确地提取手势图像块。并利用手掌面的法向量估计有效地了抑制旋转、形变等干扰。在此基础上，基于概率的局部多项式回归算法能高质量地修复深度图像，大大减少噪声对识别准确性的影响。本研究在结合了超像素分割、手势结构和EMD距离的基础上，提出了一种新的手势表示形式（超像素图）和一种新的距离度量标准（基于标准化超像素图的EMD距离）。基于此研究结果，所构建的手势识别算法，对训练数据依赖度低，同时识别精度高。在五个公开数据集上，与多个最新算法进行比较，取得了最好的识别准确率。更进一步的是，本研究利用GPU加速等方法，实现了研究的手势识别算法的实时化，最终实现了两个实际应用的演示分别是1) 五指机器人灵巧手的镜像操控和2) 三维场景漫游。本研究对手势识别方法进行细致而全面的研究，为基于手势的人机交互系统迫切需要解决 的问题提供理论支持，有着重要的科学意义和应用前景。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2-4路360度环视技术</title>
      <link>https://cwang-nbu.github.io/project/adas-project/</link>
      <pubDate>Tue, 01 Nov 2016 00:00:00 +0000</pubDate>
      <guid>https://cwang-nbu.github.io/project/adas-project/</guid>
      <description>&lt;p&gt;&lt;strong&gt;参与“北大深研院”和“深国科”校企合作项目&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;随着经济社会的发展，汽车越来越普及，人们对于汽车的安全性要求也随之提高。汽车的安全性能提高不仅有利于驾驶员、乘客，也间接地对城市的道路交通状况有帮助。2-4路360度环视技术作为驾驶员驾车的辅助技术，给驾驶员一个新的视角：高空鸟瞰。通过高空鸟瞰的视角对于车辆四周360度环绕有非常清楚的认识，进而可以帮助驾驶员更好的分析、判断路况，特别是在倒车入库、低速行车过程中，360度环视技术可以显著提高行车安全性。&lt;/p&gt;
&lt;p&gt;通过安装在车身上的2-4个鱼眼摄像头，获得前后左右2-4个方向的图片。对于这2-4个图片进行鱼眼矫正、视角转换、图像色差处理、环视合成这2-4个步骤，得到最终的环视图并显示在驾驶员的屏幕上，辅助驾驶员判断车子附近的情况。目前以opencv等开源的图像库为基础，构建、优化我们的360度环视系统。在此技术基础上，可以引入更多技术：图像显示方面，把2D显示转换为3D；速度方面：通过GPU加速计算，使得每一秒处理的图片更多，使得环视系统更有实时性，这些是我们增强360度环视系统的方向。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Example Talk</title>
      <link>https://cwang-nbu.github.io/talk/example/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate>
      <guid>https://cwang-nbu.github.io/talk/example/</guid>
      <description>&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Academic&amp;rsquo;s &lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/a&gt; feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further talk details can easily be added to this page using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>祝贺车皆咏、龚益玲和郁盛浩的论文被ICME 2022 (CCF-B) 接收</title>
      <link>https://cwang-nbu.github.io/post/202203_icme2022/</link>
      <pubDate>Sun, 06 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://cwang-nbu.github.io/post/202203_icme2022/</guid>
      <description>&lt;p&gt;&lt;strong&gt;恭喜车皆咏和龚益玲成功上岸，郁盛浩毕业了再水一篇&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;如果Accept List (&lt;a href=&#34;http://2022.ieeeicme.org/assets/accept_list.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://2022.ieeeicme.org/assets/accept_list.pdf&lt;/a&gt;) 没出错的话，居然三篇全中，这次摸奖的运气真的太好了。&lt;/p&gt;
&lt;p&gt;投稿前时间太紧，有2篇都没有细改，论文的实际水平还是有欠缺的。&lt;/p&gt;
&lt;p&gt;论文 ：&lt;/p&gt;
&lt;p&gt;1401： MULTI-SCALE CONTINUITY-AWARE REFINEMENT NETWORK FOR WEAKLY SUPERVISED VIDEO ANOMALY DETECTION&lt;/p&gt;
&lt;p&gt;1405： TCA-VAD: TEMPORAL CONTEXT ALIGNMENT NETWORK FOR WEAKLY SUPERVISED VIDEO ANOMLY DETECTION&lt;/p&gt;
&lt;p&gt;1430： PRUNING DYNAMIC GROUP CONVOLUTION WITH STATIC SUBSTITUTE&lt;/p&gt;
&lt;p&gt;紧接的3月底4月初的ACM MM 2022 (CCF-A)要继续积极参与摸奖了，后续大家投顶会和Trans应该也会更有信心了。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Balanced Stripe-wise Pruning in the Filter</title>
      <link>https://cwang-nbu.github.io/publication/conference-paper-icassp2022-2/</link>
      <pubDate>Fri, 25 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://cwang-nbu.github.io/publication/conference-paper-icassp2022-2/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Novel Instance Mining with Pseudo-Margin Evaluation for Few-Shot Object Detection</title>
      <link>https://cwang-nbu.github.io/publication/conference-paper-icassp2022-1/</link>
      <pubDate>Fri, 25 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://cwang-nbu.github.io/publication/conference-paper-icassp2022-1/</guid>
      <description></description>
    </item>
    
    <item>
      <title>祝贺刘伟杰和霍铮的论文被ICASSP 2022 (CCF-B) 接收</title>
      <link>https://cwang-nbu.github.io/post/202202_icassp2022/</link>
      <pubDate>Fri, 25 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://cwang-nbu.github.io/post/202202_icassp2022/</guid>
      <description>&lt;p&gt;&lt;strong&gt;恭喜刘伟杰和霍铮成功上岸&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;论文 ：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;“Balanced Stripe-wise Pruning in the Filter”&lt;/li&gt;
&lt;li&gt;“Novel Instance Mining with Pseudo-Margin Evaluation for Few-Shot Object Detection”&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;被 ICASSP 2022 （影响力最大的CCF-B类会议之一）接收&lt;/p&gt;
&lt;p&gt;今年投3中2，再一次蹭一下NLP领域的顶会&lt;/p&gt;
&lt;p&gt;等待3月ICME 2022 (CCF-B)的3篇摸奖的结果&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conference Rank: Computer Science (AMiner)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;排名 会议名称&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;15   [ICASSP]IEEE International Conference on Acoustics, Speech and Signal Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;H5指数 上升指数 TK5指数 CCF等级 基础研究指数 THU等级&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;86	63	32.5	B	8.01	B&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>恭喜五位同学获得宁波大学智能基座产教融合协同育人基地奖学金</title>
      <link>https://cwang-nbu.github.io/post/202201_%E5%8D%8E%E4%B8%BA%E5%A5%96%E5%AD%A6%E9%87%91/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://cwang-nbu.github.io/post/202201_%E5%8D%8E%E4%B8%BA%E5%A5%96%E5%AD%A6%E9%87%91/</guid>
      <description>&lt;p&gt;&lt;strong&gt;宁波大学 智能基座产教融合协同育人基地奖学金&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;一等奖：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;陈魏魏&lt;/strong&gt;  昇腾/鲲鹏	
1.经确认，学生参与鲲鹏BoostKit分布式存储使能套件、Linux基础入门和帮助课程学习
2.众智计划：参与王翀老师的Yolov3-tiny的pytorch开发
3.微认证：使用昇腾弹性云服务器实现目标检测应用，基于昇腾AI处理器的算子开发&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;二等奖：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;龚益玲&lt;/strong&gt;  昇腾/鲲鹏	
1.经确认，学生参与了Linux基础入门和帮助课程学习.
2.众智计划：参与王翀老师组的华为众智项目C3D模型的训练迁移。
3.微认证：使用昇腾弹性云服务器实现目标检测应用的微认证和鲲鹏iSula容器解决方案介绍与实践的微认证。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;陈松&lt;/strong&gt;	昇腾	
1.经确认，学生参与《SQL语法进阶》《Python编程学习路径》课程学习；
2.众智计划：参加王翀老师的项目“c3d模型推理”“YOLOv3-tiny模型推理”；
3.微认证：基于鲲鹏搭建zabbix分布式监控系统；使用昇腾弹性云服务器实现目标检测应用；
4.智能基座社团 副社长 负责统筹办公室日常事务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;三等奖：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;郑瑜杰&lt;/strong&gt;	昇腾/鲲鹏	
1.经确认，该同学参与《AI基础课程—Python编程知识》和《AI基础课程—常用框架工具》课程学习。
3.众智计划：参与王翀老师的项目“C3D模型端到端推理”。
4.微认证：iSula容器解决方案介绍与实践；使用昇腾弹性云服务器实现目标检测应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;陶晨晨&lt;/strong&gt;	昇腾	
1.经确认，学生参与《Linux基础入门和帮助》《Python编程学习路径》课程学习；
3众智计划：参加王翀老师的项目“YOLOv3-tiny模型推理”；
4.微认证：基于鲲鹏搭建zabbix分布式监控系统；使用昇腾弹性云服务器实现目标检测应用。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>恭喜参与华为昇腾众智项目的同学获得金质量奖</title>
      <link>https://cwang-nbu.github.io/post/202112_%E5%8D%8E%E4%B8%BA%E9%87%91%E8%B4%A8%E9%87%8F%E5%A5%96/</link>
      <pubDate>Fri, 10 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://cwang-nbu.github.io/post/202112_%E5%8D%8E%E4%B8%BA%E9%87%91%E8%B4%A8%E9%87%8F%E5%A5%96/</guid>
      <description>&lt;p&gt;&lt;strong&gt;昇腾众智金质量奖展播 2021年第四期：Go For It!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;恭喜拿到奖励的龚益玲和郑瑜杰（陈松代领：）。&lt;/p&gt;
&lt;p&gt;另一组（陈魏魏、陶晨晨、陈松）更有挑战，也顺利完成，可惜时间差了一点点。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2021年研究生聚餐</title>
      <link>https://cwang-nbu.github.io/post/202111_%E7%A0%94%E7%A9%B6%E7%94%9F%E8%81%9A%E9%A4%90/</link>
      <pubDate>Sat, 20 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://cwang-nbu.github.io/post/202111_%E7%A0%94%E7%A9%B6%E7%94%9F%E8%81%9A%E9%A4%90/</guid>
      <description>&lt;p&gt;&lt;strong&gt;一年一度的聚餐&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;今年学生规模大幅增长+全员到齐，去年的自助餐已经吃不起了&lt;/p&gt;
&lt;p&gt;今年吃高性价比日料，一个包厢已经坐不下了。&lt;/p&gt;
&lt;p&gt;明年吃什么更难定了。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>全实验室参加视觉与学习青年学者研讨会</title>
      <link>https://cwang-nbu.github.io/post/202110_valse2021/</link>
      <pubDate>Sun, 10 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://cwang-nbu.github.io/post/202110_valse2021/</guid>
      <description>&lt;p&gt;&lt;strong&gt;全团队学术活动&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;时隔2年，有一次全实验室参加VALSE 2021，声势浩大。&lt;/p&gt;
&lt;p&gt;不能出国参加国际会议的现在，对所有同学都是一次很好的经验，希望对每个人都有帮助和启发。&lt;/p&gt;
&lt;p&gt;白天听大佬讲座，晚上撸烤串，睡五星酒店。&lt;/p&gt;
&lt;p&gt;2022年在青岛可以再来一次。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cross-Epoch Learning for Weakly Supervised Anomaly Detection in Surveillance Videos</title>
      <link>https://cwang-nbu.github.io/publication/journal-article-spl2021/</link>
      <pubDate>Fri, 01 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://cwang-nbu.github.io/publication/journal-article-spl2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>祝贺郁盛浩论文被IEEE Signal Processing Letters接收</title>
      <link>https://cwang-nbu.github.io/post/202110_%E9%83%81%E7%9B%9B%E6%B5%A9spl/</link>
      <pubDate>Fri, 01 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://cwang-nbu.github.io/post/202110_%E9%83%81%E7%9B%9B%E6%B5%A9spl/</guid>
      <description>&lt;p&gt;&lt;strong&gt;恭喜郁盛浩&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;论文 “Cross-Epoch Learning for Weakly Supervised Anomaly Detection in Surveillance Videos” 被 ISCAS 2020 和 IEEE Signal Processing Letters 接收。&lt;/p&gt;
&lt;p&gt;可以安心整理毕业论文，找个996的好工作了。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Extended Guided Image Filtering For Contrast Enhancement</title>
      <link>https://cwang-nbu.github.io/publication/conference-paper-icmew2021/</link>
      <pubDate>Mon, 05 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://cwang-nbu.github.io/publication/conference-paper-icmew2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>2021年毕业生聚餐</title>
      <link>https://cwang-nbu.github.io/post/202106_%E6%AF%95%E4%B8%9A%E9%A4%90/</link>
      <pubDate>Wed, 23 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://cwang-nbu.github.io/post/202106_%E6%AF%95%E4%B8%9A%E9%A4%90/</guid>
      <description>&lt;p&gt;&lt;strong&gt;欢迎新同学加入团队&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;首届毕业生请吃饭，大家表情都有点呆滞 &amp;gt;_&amp;lt;&lt;/p&gt;
&lt;p&gt;毛乔梅在整个研究生期间对基础理论知识和相关编程能力的学习和掌握均较好，在研究过程中能积极探索新的思路和方案，思维活跃沟通顺畅，自主性和条理性较强，在实验和论文遇到困难时也能沉下心来耐心攻克。因此，其在研究生三年内获得多项荣誉和奖学金，在同期学生中也是佼佼者。在京东算法部门进行实习后，对自我的要求更上了一层楼，希望正式工作后能继续够发挥自己所长，不断学习进取，取得更好的成果。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2021年研究生招生</title>
      <link>https://cwang-nbu.github.io/post/202103_%E7%A0%94%E7%A9%B6%E7%94%9F%E6%8B%9B%E7%94%9F/</link>
      <pubDate>Tue, 23 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://cwang-nbu.github.io/post/202103_%E7%A0%94%E7%A9%B6%E7%94%9F%E6%8B%9B%E7%94%9F/</guid>
      <description>&lt;p&gt;&lt;strong&gt;欢迎新同学加入团队&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;今年名额2+1+1个：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;郑瑜杰&lt;/li&gt;
&lt;li&gt;陶晨晨&lt;/li&gt;
&lt;li&gt;戴鑫淼&lt;/li&gt;
&lt;li&gt;蒋腾辉&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Reweighted Dynamic Group Convolution</title>
      <link>https://cwang-nbu.github.io/publication/conference-paper-icassp2021/</link>
      <pubDate>Sun, 21 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://cwang-nbu.github.io/publication/conference-paper-icassp2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>祝贺陈魏魏ICASSP 2021论文接收</title>
      <link>https://cwang-nbu.github.io/post/202103_%E9%99%88%E9%AD%8F%E9%AD%8Ficassp/</link>
      <pubDate>Mon, 01 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://cwang-nbu.github.io/post/202103_%E9%99%88%E9%AD%8F%E9%AD%8Ficassp/</guid>
      <description>&lt;p&gt;&lt;strong&gt;恭喜陈魏魏&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;论文 “Reweighted Dynamic Group Convolution” 被 ICASSP 2021 （CCF-B类）接收&lt;/p&gt;
&lt;p&gt;做图像的蹭一下NLP领域的顶会&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2020年研究生聚餐</title>
      <link>https://cwang-nbu.github.io/post/202011_%E7%A0%94%E7%A9%B6%E7%94%9F%E8%81%9A%E9%A4%90/</link>
      <pubDate>Sat, 21 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://cwang-nbu.github.io/post/202011_%E7%A0%94%E7%A9%B6%E7%94%9F%E8%81%9A%E9%A4%90/</guid>
      <description>&lt;p&gt;&lt;strong&gt;一年一度的聚餐&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;去年的烤肉店已经倒闭了，今年在华侨豪生吃自助，可惜毛乔梅和陈魏魏缺席。&lt;/p&gt;
&lt;p&gt;以后要好好拍一张照片。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2020年研究生奖学金</title>
      <link>https://cwang-nbu.github.io/post/202010_%E5%A5%96%E5%AD%A6%E9%87%91/</link>
      <pubDate>Sun, 01 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://cwang-nbu.github.io/post/202010_%E5%A5%96%E5%AD%A6%E9%87%91/</guid>
      <description>&lt;p&gt;&lt;strong&gt;恭喜毛乔梅获得国家奖学金&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;其他同学也要加油!&lt;/p&gt;
&lt;p&gt;ps.
中央财政出资设立研究生国家奖学金，用于奖励普通高等学校(“国家奖学金”荣誉证书“国家奖学金”荣誉证书(2张)以下简称高等学校)中表现优异的全日制研究生。研究生国家奖学金每年奖励4.5万名在读研究生。其中，博士研究生1万名，硕士研究生为3.5万名。博士研究生国家奖学金奖励标准定为每生每年3万元；硕士研究生国家奖学金奖励标准定为每生每年2万元&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Unified Approach for Target Direction Finding based on Convolutional Neural Networks</title>
      <link>https://cwang-nbu.github.io/publication/conference-paper-mlsp2020/</link>
      <pubDate>Mon, 21 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://cwang-nbu.github.io/publication/conference-paper-mlsp2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>2020年研究生招生</title>
      <link>https://cwang-nbu.github.io/post/202005_%E7%A0%94%E7%A9%B6%E7%94%9F%E6%8B%9B%E7%94%9F/</link>
      <pubDate>Mon, 18 May 2020 00:00:00 +0000</pubDate>
      <guid>https://cwang-nbu.github.io/post/202005_%E7%A0%94%E7%A9%B6%E7%94%9F%E6%8B%9B%E7%94%9F/</guid>
      <description>&lt;p&gt;&lt;strong&gt;欢迎新同学加入团队&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;霍铮，陈松，龚益玲&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Zero-Shot Object Detection with Attributes based Category Similarity</title>
      <link>https://cwang-nbu.github.io/publication/journal-article-acs-zsd/</link>
      <pubDate>Fri, 01 May 2020 00:00:00 +0000</pubDate>
      <guid>https://cwang-nbu.github.io/publication/journal-article-acs-zsd/</guid>
      <description>&lt;p&gt;&lt;strong&gt;提取码: UxQp&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>祝贺毛乔梅ISCAS接收并推荐TCAS-II</title>
      <link>https://cwang-nbu.github.io/post/202003_%E6%AF%9B%E4%B9%94%E6%A2%85tcasii/</link>
      <pubDate>Sun, 01 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://cwang-nbu.github.io/post/202003_%E6%AF%9B%E4%B9%94%E6%A2%85tcasii/</guid>
      <description>&lt;p&gt;&lt;strong&gt;恭喜毛乔梅&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;论文 “Zero-Shot Object Detection with Attributes based Category Similarity” 被 ISCAS 2020 和 IEEE Transactions on Circuits and Systems II: Express Briefs 接收。&lt;/p&gt;
&lt;p&gt;可以去西班牙旅游了！&lt;/p&gt;
&lt;p&gt;可惜疫情推迟到10月了，希望能成行。&lt;/p&gt;
&lt;p&gt;改成线上会议了，等着录视频。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Short-Term Temporal Convolutional Networks for Dynamic Hand Gesture Recognition</title>
      <link>https://cwang-nbu.github.io/publication/preprint-2019/</link>
      <pubDate>Sat, 07 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://cwang-nbu.github.io/publication/preprint-2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>开始在英国谢菲尔德大学访问的一年</title>
      <link>https://cwang-nbu.github.io/post/201909_uklife/</link>
      <pubDate>Thu, 05 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://cwang-nbu.github.io/post/201909_uklife/</guid>
      <description>&lt;p&gt;&lt;strong&gt;简单记录下在谢菲尔德科研生活&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;周期一：2019-09-02 至 2019-12-02&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;本阶段学习和研修基本情况&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;本人于2019年9月2日抵达英国伦敦，9月3日抵达留学单位（谢菲尔德大学，University of Sheffield），并在当日中午与国外导师（刘伟博士）见面进行了初步的讨论，当日下午和电子电机工程系的系主任秘书Ms Kim Brechin见面，完成了相关报道手续。之后，在9月中上旬，完成了租房、警局注册、设备安置等生活和研修相关的安顿工作，并向曼彻斯特总领馆教育处报道。&lt;/p&gt;
&lt;p&gt;在研修过程中，与国外导师（刘伟博士）和其课题组内的蒋萌迪博士多次见面，讨论了合作研究的方向和大体思路，打算将我正在进行的深度学习相关的内容和刘伟博士研究组的阵列信号处理相结合，既能利用深度学习提高阵列信号处理的性能，也能将阵列信号相关内容应用于我在研究的图像识别领域。在9月至11月，主要进行了以下研修内容，&lt;/p&gt;
&lt;p&gt;a)	对阵列信号处理相关的内容进行了了解和学习，部分内容已在研究生阶段的研究中有所涉及，但仍有不少概念和数学上的知识需要补充。&lt;/p&gt;
&lt;p&gt;b)	仔细研读了与我们讨论决定的研究方向相近的发表于IEEE Transactions on Antennas and Propagation的论文”Direction-of-Arrival Estimation Based on Deep Neural Networks With Robustness to Array Imperfections”，经过讨论决定参考其框架进行后续研究。&lt;/p&gt;
&lt;p&gt;c)	完成了一个初步的简单神经网络的构建，包括一个单输入多输出的AutoEncoder用于DOA输入信号的分离，正在生成和调整数据以对网络进行学习和测试。&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;strong&gt;发表的论文及专利等科研成果&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;由于刚抵达英国，合作相关的研究刚处于起步阶段，还没有发表的论文等科研成果。在9月至11月期间，继续之前在国内的研究，指导学生撰写和投稿了2篇会议论文和1篇期刊论文，具体如下：&lt;/p&gt;
&lt;p&gt;a)	“Action Recognition with Temporal Selection”投稿至CCF C类会议ISCAS 2020。&lt;/p&gt;
&lt;p&gt;b)	“Zero-shot Object Detection with Attributes based Category Similarity” 投稿至CCF C类会议ISCAS 2020。&lt;/p&gt;
&lt;p&gt;c)	“Partial Person Re-identification with Pose-guided Alignment ”投稿至SCI期刊Applied Intelligence。&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;&lt;strong&gt;参加学术活动情况&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;由于有不少时间用于生活上的安顿，了解学校设施和规章制度，以及研究计划的讨论和安排，这一研修周期中并没有参加其他的学术活动。不过计划参加12月12日的MAPP Lecture，由来自剑桥大学的Dr Phillip Stanley-Marbell作题为Augmenting Raw Materials with Sensing and Computation的报告。&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;&lt;strong&gt;接下来的研修计划&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;接下来的研究主要针对讨论确定的研究路线进行展开，预计完成以下工作，&lt;/p&gt;
&lt;p&gt;a)	实现1.b)中所提的TAP论文中的整个网络并进行改进，即使用VAE替代AutoEncoder，并将后半的网络用CNN替换论文中使用的MLP，同时完成DOA数据的生成和训练。&lt;/p&gt;
&lt;p&gt;b)	将复数和四元数的计算结合进CNN中，参考Github上已经有的部分代码和实现(github.com/wavefrontshaping/complexPyTorch 等)，用于阵列信号的处理。&lt;/p&gt;
&lt;p&gt;c)	在2020年2月，与国外导师（刘伟博士）和其课题组内的蒋萌迪博士等合作撰写会议论文并投稿。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;周期二：2019-12-02 至 2020-03-02&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;本阶段学习和研修基本情况&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在本阶段研修过程中，着重进行了将深度学习应用于阵列信号方向估计的问题。与国外导师（刘伟博士）和其课题组内的蒋萌迪博士多次见面，讨论了具体的数据形式、阵列选择、深度神经网络模型等。主要工作集中在生成各类数据、设计和搭建神经网络框架，训练网络模型，并且验证实验结果。在12月至2月，主要进行了以下研修内容，&lt;/p&gt;
&lt;p&gt;a)	生成了大量Uniform Linear Array（ULA）和CoPrime Array的阵列信号训练数据。包括&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;0至90度的1-12个随机信号源的10个阵元的ULA信号的100万组covariance matrix训练数据，1万组测试数据，&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;-90至90度的1-16个随机信号源的12个阵元的CoPrime Array信号的100万，200万和1000万的covariance matrix训练数据（512个采样）各2组，10万的验证数据和测试数据各2组，&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;-60至60度的1-16个随机信号源的12个阵元的Co-Prime Array信号的200万和500万的covariance matrix以及原始信号（12，16，24个采样各一组）的训练数据各2组，10万的验证数据和测试数据各2组。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;b)	针对生成的训练、验证和测试数据，我设计了传统的30层CNN进行初步测试，得到一些初步结果。然后在ResNeXt-50和ResNeXt-101的基础上，对网络结果进行修改，并使用a)中生成的各个训练数据集进行训练，利用验证数据集确定训练论述，使用测试数据集进行测试。取得了各个实验设置下的结果，最好的512采样的数据集在限制入射角度为-60至60度时，能够达到99.9%以上的准确度。&lt;/p&gt;
&lt;p&gt;c)	根据现有数据集的结果，发现样本数对结果影响较大，covariance matrix结果要好于原始信号。同时，在检查训练过程中的validation loss时，发现有过拟合的现象。正在对网路、数据、训练过程等进行调整。&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;strong&gt;发表的论文及专利等科研成果&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在12月至2月期间，继续在国内的研究，指导学生撰写和投稿了2篇会议论文和1篇期刊论文，具体如下：&lt;/p&gt;
&lt;p&gt;a)	 “Zero-shot Object Detection with Attributes based Category Similarity” 被CCF C类会议ISCAS 2020作为Oral接收。&lt;/p&gt;
&lt;p&gt;b)	作为优秀ISCAS 2020论文收到邀请，修改和扩展了“Zero-shot Object Detection with Attributes based Category Similarity”这篇论文，投稿至TCAS-II Special Issue。&lt;/p&gt;
&lt;p&gt;c)	“Action Recognition with Temporal Selection”被ISCAS 2020拒稿，经过大幅度修改后投稿至CCF C类会议ICIP 2020。&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;&lt;strong&gt;参加学术活动情况&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在这一研修周期中参加了以下学术活动，&lt;/p&gt;
&lt;p&gt;a)	参加了University of Bristol 的Dr Ejay Nsugbe 的关于&amp;quot;Upper Limb Prosthesis for Trans-humeral Amputees&amp;quot;的讲座，&lt;/p&gt;
&lt;p&gt;b)	参加了Dr Kevin Li Sun的关于 &amp;ldquo;Deep Learning for the Robot to Think Fast&amp;rdquo;，&lt;/p&gt;
&lt;p&gt;c)	参加了Dr Nicolas Herzig的关于&amp;quot;Compliance in Robotics: A Journey through Human-Robot Environment Interactions&amp;quot;的讲座，&lt;/p&gt;
&lt;p&gt;d)	参加了Engineering You&amp;rsquo;re Hired (EYH)的活动，和当地学生进行交流。&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;&lt;strong&gt;接下来的研修计划&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;接下来的研究主要针对讨论确定的研究路线进行展开，预计完成以下工作，&lt;/p&gt;
&lt;p&gt;a)	通过更大的数据集、不同网路结构等，对模型的过拟合进行处理。&lt;/p&gt;
&lt;p&gt;b)	对结果进行整理，与国外导师（刘伟博士）和其课题组内的蒋萌迪博士等合作撰写论文并投稿。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>开始在英国谢菲尔德大学访问的一年</title>
      <link>https://cwang-nbu.github.io/post/201909_%E8%8B%B1%E5%9B%BD%E8%AE%BF%E5%AD%A6/</link>
      <pubDate>Thu, 05 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://cwang-nbu.github.io/post/201909_%E8%8B%B1%E5%9B%BD%E8%AE%BF%E5%AD%A6/</guid>
      <description>&lt;p&gt;&lt;strong&gt;简单记录下在谢菲尔德科研生活&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;周期一：2019-09-02 至 2019-12-02&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;本阶段学习和研修基本情况&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;本人于2019年9月2日抵达英国伦敦，9月3日抵达留学单位（谢菲尔德大学，University of Sheffield），并在当日中午与国外导师（刘伟博士）见面进行了初步的讨论，当日下午和电子电机工程系的系主任秘书Ms Kim Brechin见面，完成了相关报道手续。之后，在9月中上旬，完成了租房、警局注册、设备安置等生活和研修相关的安顿工作，并向曼彻斯特总领馆教育处报道。&lt;/p&gt;
&lt;p&gt;在研修过程中，与国外导师（刘伟博士）和其课题组内的蒋萌迪博士多次见面，讨论了合作研究的方向和大体思路，打算将我正在进行的深度学习相关的内容和刘伟博士研究组的阵列信号处理相结合，既能利用深度学习提高阵列信号处理的性能，也能将阵列信号相关内容应用于我在研究的图像识别领域。在9月至11月，主要进行了以下研修内容，&lt;/p&gt;
&lt;p&gt;a)	对阵列信号处理相关的内容进行了了解和学习，部分内容已在研究生阶段的研究中有所涉及，但仍有不少概念和数学上的知识需要补充。&lt;/p&gt;
&lt;p&gt;b)	仔细研读了与我们讨论决定的研究方向相近的发表于IEEE Transactions on Antennas and Propagation的论文”Direction-of-Arrival Estimation Based on Deep Neural Networks With Robustness to Array Imperfections”，经过讨论决定参考其框架进行后续研究。&lt;/p&gt;
&lt;p&gt;c)	完成了一个初步的简单神经网络的构建，包括一个单输入多输出的AutoEncoder用于DOA输入信号的分离，正在生成和调整数据以对网络进行学习和测试。&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;strong&gt;发表的论文及专利等科研成果&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;由于刚抵达英国，合作相关的研究刚处于起步阶段，还没有发表的论文等科研成果。在9月至11月期间，继续之前在国内的研究，指导学生撰写和投稿了2篇会议论文和1篇期刊论文，具体如下：&lt;/p&gt;
&lt;p&gt;a)	“Action Recognition with Temporal Selection”投稿至CCF C类会议ISCAS 2020。&lt;/p&gt;
&lt;p&gt;b)	“Zero-shot Object Detection with Attributes based Category Similarity” 投稿至CCF C类会议ISCAS 2020。&lt;/p&gt;
&lt;p&gt;c)	“Partial Person Re-identification with Pose-guided Alignment ”投稿至SCI期刊Applied Intelligence。&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;&lt;strong&gt;参加学术活动情况&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;由于有不少时间用于生活上的安顿，了解学校设施和规章制度，以及研究计划的讨论和安排，这一研修周期中并没有参加其他的学术活动。不过计划参加12月12日的MAPP Lecture，由来自剑桥大学的Dr Phillip Stanley-Marbell作题为Augmenting Raw Materials with Sensing and Computation的报告。&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;&lt;strong&gt;接下来的研修计划&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;接下来的研究主要针对讨论确定的研究路线进行展开，预计完成以下工作，&lt;/p&gt;
&lt;p&gt;a)	实现1.b)中所提的TAP论文中的整个网络并进行改进，即使用VAE替代AutoEncoder，并将后半的网络用CNN替换论文中使用的MLP，同时完成DOA数据的生成和训练。&lt;/p&gt;
&lt;p&gt;b)	将复数和四元数的计算结合进CNN中，参考Github上已经有的部分代码和实现(github.com/wavefrontshaping/complexPyTorch 等)，用于阵列信号的处理。&lt;/p&gt;
&lt;p&gt;c)	在2020年2月，与国外导师（刘伟博士）和其课题组内的蒋萌迪博士等合作撰写会议论文并投稿。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;周期二：2019-12-02 至 2020-03-02&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;本阶段学习和研修基本情况&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在本阶段研修过程中，着重进行了将深度学习应用于阵列信号方向估计的问题。与国外导师（刘伟博士）和其课题组内的蒋萌迪博士多次见面，讨论了具体的数据形式、阵列选择、深度神经网络模型等。主要工作集中在生成各类数据、设计和搭建神经网络框架，训练网络模型，并且验证实验结果。在12月至2月，主要进行了以下研修内容，&lt;/p&gt;
&lt;p&gt;a)	生成了大量Uniform Linear Array（ULA）和CoPrime Array的阵列信号训练数据。包括&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;0至90度的1-12个随机信号源的10个阵元的ULA信号的100万组covariance matrix训练数据，1万组测试数据，&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;-90至90度的1-16个随机信号源的12个阵元的CoPrime Array信号的100万，200万和1000万的covariance matrix训练数据（512个采样）各2组，10万的验证数据和测试数据各2组，&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;-60至60度的1-16个随机信号源的12个阵元的Co-Prime Array信号的200万和500万的covariance matrix以及原始信号（12，16，24个采样各一组）的训练数据各2组，10万的验证数据和测试数据各2组。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;b)	针对生成的训练、验证和测试数据，我设计了传统的30层CNN进行初步测试，得到一些初步结果。然后在ResNeXt-50和ResNeXt-101的基础上，对网络结果进行修改，并使用a)中生成的各个训练数据集进行训练，利用验证数据集确定训练论述，使用测试数据集进行测试。取得了各个实验设置下的结果，最好的512采样的数据集在限制入射角度为-60至60度时，能够达到99.9%以上的准确度。&lt;/p&gt;
&lt;p&gt;c)	根据现有数据集的结果，发现样本数对结果影响较大，covariance matrix结果要好于原始信号。同时，在检查训练过程中的validation loss时，发现有过拟合的现象。正在对网路、数据、训练过程等进行调整。&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;strong&gt;发表的论文及专利等科研成果&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在12月至2月期间，继续在国内的研究，指导学生撰写和投稿了2篇会议论文和1篇期刊论文，具体如下：&lt;/p&gt;
&lt;p&gt;a)	 “Zero-shot Object Detection with Attributes based Category Similarity” 被CCF C类会议ISCAS 2020作为Oral接收。&lt;/p&gt;
&lt;p&gt;b)	作为优秀ISCAS 2020论文收到邀请，修改和扩展了“Zero-shot Object Detection with Attributes based Category Similarity”这篇论文，投稿至TCAS-II Special Issue。&lt;/p&gt;
&lt;p&gt;c)	“Action Recognition with Temporal Selection”被ISCAS 2020拒稿，经过大幅度修改后投稿至CCF C类会议ICIP 2020。&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;&lt;strong&gt;参加学术活动情况&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在这一研修周期中参加了以下学术活动，&lt;/p&gt;
&lt;p&gt;a)	参加了University of Bristol 的Dr Ejay Nsugbe 的关于&amp;quot;Upper Limb Prosthesis for Trans-humeral Amputees&amp;quot;的讲座，&lt;/p&gt;
&lt;p&gt;b)	参加了Dr Kevin Li Sun的关于 &amp;ldquo;Deep Learning for the Robot to Think Fast&amp;rdquo;，&lt;/p&gt;
&lt;p&gt;c)	参加了Dr Nicolas Herzig的关于&amp;quot;Compliance in Robotics: A Journey through Human-Robot Environment Interactions&amp;quot;的讲座，&lt;/p&gt;
&lt;p&gt;d)	参加了Engineering You&amp;rsquo;re Hired (EYH)的活动，和当地学生进行交流。&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;&lt;strong&gt;接下来的研修计划&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;接下来的研究主要针对讨论确定的研究路线进行展开，预计完成以下工作，&lt;/p&gt;
&lt;p&gt;a)	通过更大的数据集、不同网路结构等，对模型的过拟合进行处理。&lt;/p&gt;
&lt;p&gt;b)	对结果进行整理，与国外导师（刘伟博士）和其课题组内的蒋萌迪博士等合作撰写论文并投稿。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>https://cwang-nbu.github.io/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://cwang-nbu.github.io/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-academic&#34;&gt;Create slides in Markdown with Academic&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Academic&lt;/a&gt; | &lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;span class=&#34;fragment &#34; &gt;
  One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
  &lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
  Three
&lt;/span&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/img/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/img/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://spectrum.chat/academic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An Improved Guided Filtering Algorithm for Image Enhancement</title>
      <link>https://cwang-nbu.github.io/publication/conference-paper-icme2018/</link>
      <pubDate>Mon, 23 Jul 2018 00:00:00 +0000</pubDate>
      <guid>https://cwang-nbu.github.io/publication/conference-paper-icme2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Superpixel-based color-depth restoration and dynamic environment modeling for Kinect-assisted image-based rendering systems</title>
      <link>https://cwang-nbu.github.io/publication/journal-article-tvc2018/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://cwang-nbu.github.io/publication/journal-article-tvc2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A hand gesture recognition system based on canonical superpixel-graph</title>
      <link>https://cwang-nbu.github.io/publication/journal-article-csgemd/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 +0000</pubDate>
      <guid>https://cwang-nbu.github.io/publication/journal-article-csgemd/</guid>
      <description>&lt;p&gt;本论文获得&lt;strong&gt;2017-2018年度宁波市自然科学优秀论文奖 三等奖&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Superpixel-Based Hand Gesture Recognition With Kinect Depth Camera</title>
      <link>https://cwang-nbu.github.io/publication/journal-article-spemd/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://cwang-nbu.github.io/publication/journal-article-spemd/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An example preprint / working paper</title>
      <link>https://cwang-nbu.github.io/publication/preprint/</link>
      <pubDate>Sat, 07 Apr 2012 00:00:00 +0000</pubDate>
      <guid>https://cwang-nbu.github.io/publication/preprint/</guid>
      <description>&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
